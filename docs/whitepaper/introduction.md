---
sidebar_position: 2
---

# INTRODUCTION

## 1. We need Knowledge

The digital revolution has led to an explosive growth in data acquisition, storage and analysis, fundamentally reshaping our collective knowledge base and interactions.

This shift has transformed us from a society of physical ownership to one based **on licenses and intellectual property**, spurring the growth of **dematerialized assets**. While this offers new opportunities, it's also created information and technological power concentration within tech giants.

In light of pressing issues such as climate change and natural resource scarcity, the urgent need for open knowledge creation and sharing becomes evident.

We don’t *know* how to go to Mars, we don’t *know* how to predict cancer, we don’t *know* how to farm in a desert, and we don’t *know* what present would please our mother for her birthday.

Knowledge is the limiting factor to the common good for individuals, companies and society at large.

Knowledge, in its various forms, is a powerful catalyst for progress and innovation, impacting all areas of human endeavor from economic growth and social development to technological advancement and cultural enrichment.

- Knowledge is infinite, unlike any material good
- Knowledge is valuable, it holds the key to all problem-solving
- Knowledge is power, it can change the world for everyone's benefit

Our knowledge is inherently limited by the way it is created and shared. This creation begins with data - the raw material gathered by humans or machines through observation. This data serves as the inputs that enable us to train our algorithmic minds and interpret the world. Upon being contextualized and interpreted, this data becomes knowledge.

However, despite the omnipresence of data and algorithms, our knowledge remains limited. Why is this? **Because data and algorithms are not shared.**, essentially because of deficits of trust and incentives.

This is where the decentralized web steps in, capable of reshuffling the cards in terms of knowledge creation and dissemination. The decentralized web holds the key for a fair and trusted infrastructure to share any existing dataset and algorithms, while tokens give innovating tools to incentivize collaboration. Within this context, OKP4 sets out to capitalize on the virtues of both Web 2.0 and Web 3.0 with the goal of propelling the knowledge economy to the next level.

## 2. Why Sharing is not the Norm

Despite the context leading to the adoption of data and knowledge sharing, it is clear that **this is not at all the norm today.**

Today's prevailing issue is that individuals and organizations tend to veer towards a siloed approach when it comes to handling data and knowledge. They create isolated data lakes and restrict access to proprietary information, effectively imprisoning valuable knowledge within walled gardens. This is mainly due to:

- **Risks** (lack of trust between participants, the fear of loss of competitive advantage)
- **Costs** (lack of infrastructures, data governance and interoperability issues)
- **Lack of incentives** (bad business models, bad incentives, unclear benefits)

**Stats** :

*In terms of the costs associated with data silos, a report by McKinsey estimated that inefficiencies due to data silos and related issues could cost businesses more than $5 trillion annually (source: [**McKinsey**](https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/the-perils-of-ignoring-software-development)).*

*According to a report by IDC, the volume of data generated globally is expected to grow from 33 zettabytes in 2018 to 175 zettabytes by 2025 (source: [**IDC**](https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf)). However, it's estimated that a significant amount of this data is inaccessible or unused. For example, in the healthcare industry, as much as 80% of health data is believed to be unstructured and inaccessible (source: [**International Journal of Medical Informatics**](https://www.sciencedirect.com/science/article/pii/S1386505619301364)). This represents a significant loss of potential knowledge and insights.*

As a result of these barriers, the landscape of data and knowledge sharing has evolved in a fragmented way. Rather than a comprehensive and integrated solution, we see a **patchwork of tools and platforms**, each addressing a specific aspect or a specific stage of the knowledge value chain. Some focus on data acquisition, others on data organization and storage, yet others on data analysis and application, and still others on data sharing and dissemination. **The lack of a holistic solution** that spans the entire value chain creates inefficiencies, redundancies, and gaps.

Indeed, it has been difficult for a cohesive, comprehensive approach to data and knowledge sharing to gain traction. The existing solutions have provided **partial answers**, but they have not been able to deliver the level of **interoperability, security, and scalability required** for true data and knowledge sharing on a global scale.

Historically, we can draw parallels to the evolution of communication networks. In the early days of telecommunication, there were numerous competing systems that were unable to interconnect, resulting in isolated networks with limited reach. It wasn't until the advent of the telephone and the development of standardized protocols that a truly global communication network emerged.

The same kind of transformation is needed today in the realm of data and knowledge sharing

## 3. Current state: fragmented and complex

Recognizing the immense potential and value of data exploitation and cognition, tech giants swiftly grasped the importance of mastering the Knowledge Value Chain for their own operations. Having fine-tuned their approach to managing and leveraging data, they are now able to extend their capabilities beyond their core operations, offering sophisticated data infrastructure solutions as a product to others, thereby shaping the landscape of data and knowledge sharing in the digital realm.

- **Google Cloud Platform**:

    Google offers a range of products that cover a wide range of data and knowledge needs. For example, Google BigQuery for big data analysis, Google Cloud Storage for data storage, Google Data Studio for data visualization, and Google Workspace (formerly G Suite) for collaboration and knowledge sharing.

- **Amazon Web Services (AWS):**

    AWS provides a comprehensive suite of services for data and knowledge management. For instance, Amazon S3 for scalable storage, Amazon Redshift for data warehousing, Amazon Quicksight for data visualization, and Amazon Chime for collaboration and knowledge sharing. Additionally, they offer more specialized tools like Amazon Athena for querying large datasets and Amazon Comprehend for natural language processing and machine learning applications.

- **Microsoft Azure**:

    Azure offers a range of services for data and knowledge management. For example, Azure Data Lake for storage and analysis of Big Data, Azure SQL Database for database management, Power BI for data visualization, and Microsoft Teams for collaboration and knowledge sharing.

- **IBM Cloud**:

    IBM offers a range of services for data and knowledge management. For example, IBM Db2 for database management, IBM Watson for AI and data analysis, IBM Cognos Analytics for data visualization, and IBM Connections for knowledge sharing and collaboration.

- **Snowflake**:

    Snowflake is a fully managed cloud-based data storage and analysis platform that offers a solution for data storage, data engineering, data warehousing, data lake, data analytics, AI, and machine learning. It's important to note that although Snowflake provides a comprehensive solution for data management, it primarily focuses on structured and semi-structured aspects of data.

While these solutions leverage diverse technologies, their common goal is to establish a conducive environment for data and knowledge exploitation. They offer the convenience of a ready-to-use framework, user-friendly interfaces, and comprehensive support services, enabling seamless onboarding. However, these benefits often come with a significant price tag.

Despite their advantages, a critical limitation common to these solutions is **their centralization**. Centralization poses significant challenges, especially in the realms of data and knowledge sharing. The central issues arising due to this structure encompass:

- Confidentiality
- Security
- Interoperability
- Provider dependence
- Sovereignty

Despite their benefits, the drawbacks associated with centralized data management systems necessitate exploring alternative approaches.

### Web 3.0 Solutions

In light of data sharing, blockchain technologies play a crucial role. They can offer a secure, transparent, and decentralized infrastructure, extending their value beyond financial transactions to peer-to-peer data exchanges and secure collaborations. This is increasingly important in a data-driven world where privacy, security, and the ability to customize solutions are paramount. Hence, the relevance of blockchain technology in creating a more open and interconnected world of secure data sharing is profound, and its potential stretches across numerous sectors.

Ethereum expanded decentralization principles by introducing programmable smart contracts, enabling the development of numerous decentralized applications (dApps). However, Ethereum's broad scope comes with its own limitations. As a general-purpose platform, it may not always provide the most efficient solution for every use case.

Given the sensitivity of the data and knowledge sector, it's crucial that the infrastructure is as resilient and neutral as possible. Moreover, due to the intricacy of this field, there's a specific design requirement that fosters sharing while managing these complexities efficiently.

Specific Web 3.0 solutions are designed to offer specific functionalities such as decentralized storage, computation, indexing, and data validation for instance.

- Decentralized storage solutions like [Arweave](https://www.arweave.org/) or [Filecoin](https://filecoin.io/) offer secure and robust data storage, crucial for any data and knowledge management system.
- [The Graph](https://thegraph.com/), a decentralized indexing solution, facilitates efficient data retrieval from blockchains through a GraphQL API, simplifying developer interaction with blockchain data.
- [Kyve](https://www.kyve.network/), a decentralized data validation network, ensures data integrity and reliability, both essential for data management.
- [Akash](https://akash.network/), a decentralized marketplace to buy and sell computing resources
- [Secret Network](https://scrt.network/), offering decentralized privacy-preserving solutions such as Homomorphic Encryption
- Other notable initiatives to valorize data include for exemple [Ocean Protocol](https://oceanprotocol.com/) specializing in data markets, [DIMO](https://dimo.zone/) incentivizing vehicule data sharing or [Hivemapper] incentivizing mapping data collection

Each of these solutions **offers a unique contribution** to the landscape of decentralized digital resource management & valorization. By addressing specific challenges, they form the building blocks for a more robust, efficient, and secure Web 3.0 infrastructure.

And this is without considering the fields of Zero-Knowledge, Artificial Intelligence and advanced cryptographic methods to facilitate secure data sharing and encrypted data analysis.

In this complex context, the challenge is to **develop a holistic solution to connect all the relevant solutions** (web 2.0 & 3.0) in order to effectively manage both data and knowledge at all stages of the Knowledge Value Chain.

## 4. Challenges to open the knowledge silos

Considering the backdrop, the challenges, and the current state of solutions that we outlined in this introduction, we've critically analyzed every key aspect of data sharing to understand **how to build a successful knowledge economy.**.

This mission includes **maintaining the delicate balance** between respecting the **technical aspects** of these integrations while also addressing **concerns and incentives** of all involved parties – an interplay between human and technical factors.

To break down silos and unlock the full potential of data and algorithm sharing, the solution is to allow the creation of  **viable business models with reliable governance models.**

To reach this purpose, the challenges are divided into two distinct sections:

- **The Human Issues:**: Removing, as much as possible, the need for trust between participants through on-chain rules definition and enforcement. Creating powerful incentives to share and consume off-chain resources through the protocol.
- **The Technical Issues**: Building the actual resources, tools, and systems needed to store, process, and transfer information.

OKP4 aims to establish a system that encourages collaboration, co-creation, and knowledge sharing while ensuring and respecting individual consents and interests. As a neutral and public good, OKP4 focuses on addressing Technical solutions to provide an agnostic, flexible and secure infrastructure to solve Human Issues. The real challenge is to provide a solution that is simultaneously flexible enough to meet the needs of various stakeholders, expressive enough to interpret and interoperate different components, and secure enough to be executed within a trustworthy environment.

From a technical standpoint, OKP4 offers a unique solution, deftly combining the virtues of openness and decentralization with the adaptive potential of interoperability and modularity.

In the following section, we will explore how OKP4 elegantly combines these four pillars to elevate the knowledge economy to the next level.
