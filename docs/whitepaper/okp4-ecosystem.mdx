---
sidebar_position: 6
---

import { Mermaid } from 'mdx-mermaid/Mermaid'

# OKP4 Ecosystem

## Overview

OKP4 is a general-purpose Ecosystem that enables [XaaS](https://en.wikipedia.org/wiki/As_a_service) - Anything as a Service. Therein lies the extensibility and versatility of the protocol. Anything that is presented to the protocol *as a Service* can then be used by the protocol. In a Data Space, Services are not hosted centrally but anywhere (in the cloud or in premise), from any provider and are only shared when necessary according to strict rules.

There is a large possible typology of services and the protocol currently recognizes the following types:

- *Data* as a Service - Services allowing data management: storage, accessibility, administration, etc.
- *Algorithm* as a Service - Services that process data to produce meaningful information (i.e. knowledge), for instance: machine learning algorithms.
- *Infrastructure* as a Service - Services that offers compute, storage and networking resources.

The Ecosystem is based on a fully decentralized open and agnostic architecture, which is:

- *Scalable*, designed to scale to infinity,
- *Interoperable* to ensure integration with existing and future services,
- *Auditable*, by the fine description of the topology of the Ecosystem, by the recording in an ontology of all the activities.

## Decentralized Orchestration / Workflow engine

### The workflow engine

The workflow engine is a special service of the protocol which orchestrates the invocations of other services. It is a reflex component of the Ecosystem that listens to the transactions of the blockchain and is triggered on command, when a particular transaction of execution request is registered in the blockchain.

The following diagram describes the different possible interactions during the execution of a workflow with a single service.

<Mermaid chart={`sequenceDiagram
    autonumber
    actor A as Alice
    participant O as OKP4 blockchain
    participant W as Workflow Engine
    participant S as Service
    par
        A-)O: tx: execute-workflow(params)
    and processing loop
        loop
            W->>O: query next tx
            O-->>W: tx
            alt tx is execute-workflow
                W->>O: retrieve service metadata
                O-->>W: service metadata
                W-)O: tx: report-workflow-state
                W->>W: process service
                W->>S: call service(params)
                S-->>W: result
                W-)O: tx: report-workflow-state
            else
            end
        end
    end
    
`} />

- (1) - The use case starts with the submission of a transaction requesting to execute a workflow.
- (2) & (3) - In parallel, the workflow engine regularly queries the blockchain to retrieve the latest validated transactions.
- (4) & (5) - If the transaction is a request to execute a workflow, the workflow engine retrieves the metadata describing the service to invoke. For simplicity, we consider here that the invoked workflow consists of a single service.
- (6) - Before invoking the service, the workflow engine records a status transaction in the blockchain. This information is important because it helps to ensure that the contract established between the customer and the various suppliers (data, services, infrastructure, etc.) is honored.
- (7), (8) & (9) - Based on the service metadata, the workflow engine is able to call the service.
- (10) - After invoking the service, the workflow engine records a status transaction (success, failure) in the blockchain.

### The knowledge graph

All the different activities of the workflow engine, and especially the invocations of the services, are recorded in the blockchain, with all the necessary context for an complete understanding.

Since all the information is expressed in the form of an ontology, the workflow engine enables the chain of custody of each piece of data (*what*) during the different processings (*how*) operated on the different locations (*where*) at a given time (*when*), which leads to a knowledge graph from the source data to the final knowledge produced. Moreover, based on this knowledge graph, monetary policies can be applied in order to reward all the actors in the workflow, from data providers to service providers, in accordance with the business model stated in the Data Space.

### The workflow

A workflow is a (standardized) description of a processing flow using existing Services. It is understood that a workflow can be reduced to the use of a single service, in which case the processing is just the invocation of that service.

The services registered in the catalog in the protocol are described as a Workflow using a [Domain Specific Language](https://en.wikipedia.org/wiki/Domain-specific_language) that allows the execution of the interaction between the workflow engine and the described service with great flexibility. In this sense, the protocol service descriptions, at runtime, act as a broker to the target services, an intermediary that performs protocol adaptation and translation between the workflow engine and the invoked services.

### The decentralization of orchestration

Decentralization comes first and foremost through the protocol's ability to bring into play services of all kinds, hosted anywhere. It is also possible to have the same services (or services offering a similar set of features) deployed in several places (cloud or in premise) , which reinforces the distributed aspect. The workflow engine itself obeys the same principles and can therefore exist in different deployments in the cloud. In this case, the selection of the workflow engine instance can be done according to different strategies that can be decided at the Data Space level and/or at the level of the user running the workflow.

Secondly, decentralization is also achieved by the ability of the protocol to be agnostic: indeed, each service can be described by any language and thus interpreted by any Workflow Engine. By this principle, it is possible to have services invocable by different Workflow Engines.

## Data / Algorithms

*Work in progress*

## Storage, computation

*Work in progress*

## Mechanisms to connect/disconnect

*Work in progress*

## Datasets / algorithms / resource governance â€“ sovereignty

*Work in progress*

## Security & Risks

*Work in progress*
